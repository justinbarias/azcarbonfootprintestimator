{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import dateutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_KEY_TO_REGION = {\n",
    "    'AUS-NSW': 'NSW1',\n",
    "    'AUS-QLD': 'QLD1',\n",
    "    'AUS-SA': 'SA1',\n",
    "    'AUS-TAS': 'TAS1',\n",
    "    'AUS-VIC': 'VIC1',\n",
    "    'AUS-WA': 'WEM',\n",
    "}\n",
    "ZONE_KEY_TO_NETWORK = {\n",
    "    'AUS-NSW': 'NEM',\n",
    "    'AUS-QLD': 'NEM',\n",
    "    'AUS-SA': 'NEM',\n",
    "    'AUS-TAS': 'NEM',\n",
    "    'AUS-VIC': 'NEM',\n",
    "    'AUS-WA': 'WEM',\n",
    "}\n",
    "EXCHANGE_MAPPING_DICTIONARY = {\n",
    "    'AUS-NSW->AUS-QLD': {\n",
    "        'region_id': 'NSW1->QLD1',\n",
    "        'direction': 1,\n",
    "    },\n",
    "    'AUS-NSW->AUS-VIC': {\n",
    "        'region_id': 'NSW1->VIC1',\n",
    "        'direction': 1,\n",
    "    },\n",
    "    'AUS-SA->AUS-VIC': {\n",
    "        'region_id': 'SA1->VIC1',\n",
    "        'direction': 1,\n",
    "    },\n",
    "    'AUS-TAS->AUS-VIC': {\n",
    "        'region_id': 'TAS1->VIC1',\n",
    "        'direction': 1,\n",
    "    },\n",
    "}\n",
    "OPENNEM_PRODUCTION_CATEGORIES = {\n",
    "    'coal': ['COAL_BLACK', 'COAL_BROWN'],\n",
    "    'gas': ['GAS_CCGT', 'GAS_OCGT', 'GAS_RECIP', 'GAS_STEAM'],\n",
    "    'oil': ['DISTILLATE'],\n",
    "    'hydro': ['HYDRO'],\n",
    "    'wind': ['WIND'],\n",
    "    'biomass': ['BIOENERGY_BIOGAS', 'BIOENERGY_BIOMASS'],\n",
    "    'solar': ['SOLAR_UTILITY', 'SOLAR_ROOFTOP'],\n",
    "}\n",
    "OPENNEM_STORAGE_CATEGORIES = {\n",
    "    # Storage\n",
    "    'battery': ['BATTERY_DISCHARGING', 'BATTERY_CHARGING'],\n",
    "    'hydro': ['PUMPS'],\n",
    "}\n",
    "SOURCE = 'opennem.org.au'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_df(dataset):\n",
    "    series = dataset['history']\n",
    "    interval = series['interval']\n",
    "    dt_start = arrow.get(series['start']).datetime\n",
    "    dt_end = arrow.get(series['last']).datetime\n",
    "    data_type = dataset['data_type']\n",
    "    _id = dataset.get('id')\n",
    "\n",
    "    if data_type != 'power':\n",
    "        name = data_type.upper()\n",
    "    else:\n",
    "        # When `power` is given, the multiple power sources will be given\n",
    "        # we therefore set `name` to the power source\n",
    "        name = _id.split(\".\")[-2].upper()\n",
    "\n",
    "    # Turn into minutes\n",
    "    if interval[-1] == \"m\":\n",
    "        interval += \"in\"\n",
    "\n",
    "    index = pd.date_range(start=dt_start, end=dt_end, freq=interval)\n",
    "    assert len(index) == len(series['data'])\n",
    "    df = pd.DataFrame(index=index, data=series['data'], columns=[name])\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_url(zone_key, is_flow, target_datetime, logger) -> str:\n",
    "    if target_datetime:\n",
    "        network = ZONE_KEY_TO_NETWORK[zone_key]\n",
    "        # We will fetch since the beginning of the current month\n",
    "        month = arrow.get(target_datetime).floor('month').format('YYYY-MM-DD')\n",
    "        if is_flow:\n",
    "            url = f'http://api.opennem.org.au/stats/flow/network/{network}?month={month}'\n",
    "        else:\n",
    "            region = ZONE_KEY_TO_REGION.get(zone_key)\n",
    "            url = f'http://api.opennem.org.au/stats/power/network/fueltech/{network}/{region}?month={month}'\n",
    "    else:\n",
    "        # Contains flows and production combined\n",
    "        url = f'https://data.opennem.org.au/v3/clients/em/latest.json'\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def fetch_main_df(data_type, zone_key=None, sorted_zone_keys=None, session=None, target_datetime=None, logger=logging.getLogger(__name__)):\n",
    "    region = ZONE_KEY_TO_REGION.get(zone_key)\n",
    "    url = generate_url(\n",
    "        zone_key=zone_key or sorted_zone_keys[0],\n",
    "        is_flow=sorted_zone_keys is not None,\n",
    "        target_datetime=target_datetime,\n",
    "        logger=logger)\n",
    "\n",
    "    # Fetches the last week of data\n",
    "    logger.info(f'Requesting {url}..')\n",
    "    r = (session or requests).get(url)\n",
    "    r.raise_for_status()\n",
    "    logger.debug('Parsing JSON..')\n",
    "    datasets = r.json()['data']\n",
    "    logger.debug('Filtering datasets..')\n",
    "    filtered_datasets = [\n",
    "        ds for ds in datasets\n",
    "        if ds['type'] == data_type and (\n",
    "            (zone_key and ds.get('region') == region)\n",
    "            or (sorted_zone_keys and ds.get('id').split('.')[-2] == EXCHANGE_MAPPING_DICTIONARY['->'.join(sorted_zone_keys)]['region_id'])\n",
    "        )\n",
    "    ]\n",
    "    logger.debug('Concatenating datasets..')\n",
    "    df = pd.concat([dataset_to_df(ds) for ds in filtered_datasets], axis=1)\n",
    "\n",
    "    # Sometimes we get twice the columns. In that case, only return the first one\n",
    "    is_duplicated_column = df.columns.duplicated(keep='last')\n",
    "    if is_duplicated_column.sum():\n",
    "        logger.warning(f'Dropping columns {df.columns[is_duplicated_column]} that appear more than once')\n",
    "        df = df.loc[:, is_duplicated_column]\n",
    "\n",
    "    logger.debug('Preparing capacities..')\n",
    "    if data_type == 'power' and zone_key:\n",
    "        # SOLAR_ROOFTOP is only given at 30 min interval, so let's interpolate it\n",
    "        if 'SOLAR_ROOFTOP' in df:\n",
    "            df['SOLAR_ROOFTOP'] = df['SOLAR_ROOFTOP'].interpolate(limit=5)\n",
    "        # Parse capacity data\n",
    "        capacities = dict([\n",
    "            (obj['id'].split('.')[-2].upper(), obj.get('x_capacity_at_present'))\n",
    "            for obj in filtered_datasets if obj['region'] == region\n",
    "        ])\n",
    "        return df, pd.Series(capacities)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def sum_vector(pd_series, keys, transform=None):\n",
    "    # Only consider keys that are in the pd_series\n",
    "    filtered_keys = pd_series.index.intersection(keys)\n",
    "\n",
    "    # Require all present keys to be non-null\n",
    "    pd_series_filtered = pd_series.loc[filtered_keys]\n",
    "    if filtered_keys.size and pd_series_filtered.notnull().all():\n",
    "        if transform:\n",
    "            return pd_series_filtered.apply(transform).sum()\n",
    "        return pd_series_filtered.sum()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_production(zone_key=None, session=None, target_datetime=None, logger=logging.getLogger(__name__)):\n",
    "    df, capacities = fetch_main_df('power', zone_key=zone_key, session=session, target_datetime=target_datetime, logger=logger)\n",
    "    # Drop interconnectors\n",
    "    df = df.drop([x for x in df.columns if '->' in x], axis=1)\n",
    "\n",
    "    # Make sure charging is counted positively\n",
    "    # and discharging negetively\n",
    "    if 'BATTERY_DISCHARGING' in df.columns:\n",
    "        df['BATTERY_DISCHARGING'] = df['BATTERY_DISCHARGING'] * -1\n",
    "\n",
    "    logger.debug('Preparing final objects..')\n",
    "    objs = [{\n",
    "        'datetime': arrow.get(dt.to_pydatetime()).datetime,\n",
    "        'production': {  # Unit is MW\n",
    "            'coal': sum_vector(row, OPENNEM_PRODUCTION_CATEGORIES['coal']),\n",
    "            'gas': sum_vector(row, OPENNEM_PRODUCTION_CATEGORIES['gas']),\n",
    "            'oil': sum_vector(row, OPENNEM_PRODUCTION_CATEGORIES['oil']),\n",
    "            'hydro': sum_vector(row, OPENNEM_PRODUCTION_CATEGORIES['hydro']),\n",
    "            'wind': sum_vector(row, OPENNEM_PRODUCTION_CATEGORIES['wind']),\n",
    "            'biomass': sum_vector(row, OPENNEM_PRODUCTION_CATEGORIES['biomass']),\n",
    "            # We here assume all rooftop solar is fed to the grid\n",
    "            # This assumption should be checked and we should here only report\n",
    "            # grid-level generation\n",
    "            'solar': sum_vector(row, OPENNEM_PRODUCTION_CATEGORIES['solar']),\n",
    "        },\n",
    "        'storage': {\n",
    "            # opennem reports charging as negative, we here should report as positive\n",
    "            # Note: we made the sign switch before, so we can just sum them up\n",
    "            'battery': sum_vector(row, OPENNEM_STORAGE_CATEGORIES['battery']),\n",
    "            # opennem reports pumping as positive, we here should report as positive\n",
    "            'hydro': sum_vector(row, OPENNEM_STORAGE_CATEGORIES['hydro']),\n",
    "        },\n",
    "        'capacity': {\n",
    "            'coal': sum_vector(capacities, OPENNEM_PRODUCTION_CATEGORIES['coal']),\n",
    "            'gas': sum_vector(capacities, OPENNEM_PRODUCTION_CATEGORIES['gas']),\n",
    "            'oil': sum_vector(capacities, OPENNEM_PRODUCTION_CATEGORIES['oil']),\n",
    "            'hydro': sum_vector(capacities, OPENNEM_PRODUCTION_CATEGORIES['hydro']),\n",
    "            'wind': sum_vector(capacities, OPENNEM_PRODUCTION_CATEGORIES['wind']),\n",
    "            'biomass': sum_vector(capacities, OPENNEM_PRODUCTION_CATEGORIES['biomass']),\n",
    "            'solar': sum_vector(capacities, OPENNEM_PRODUCTION_CATEGORIES['solar']),\n",
    "            'hydro storage': capacities.get(OPENNEM_STORAGE_CATEGORIES['hydro'][0]),\n",
    "            'battery storage': capacities.get(OPENNEM_STORAGE_CATEGORIES['battery'][0]),\n",
    "        },\n",
    "        'source': SOURCE,\n",
    "        'zoneKey': zone_key,\n",
    "    } for dt, row in df.iterrows()]\n",
    "\n",
    "    # Validation\n",
    "    logger.debug('Validating..')\n",
    "    for obj in objs:\n",
    "        for k, v in obj['production'].items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if v < 0 and v > -50:\n",
    "                # Set small negative values to 0\n",
    "                logger.warning(f'Setting small value of {k} ({v}) to 0.',\n",
    "                               extra={'key': zone_key})\n",
    "                obj['production'][k] = 0\n",
    "\n",
    "    return objs\n",
    "\n",
    "\n",
    "def fetch_price(zone_key=None, session=None, target_datetime=None, logger=logging.getLogger(__name__)) -> list:\n",
    "    df = fetch_main_df('price', zone_key=zone_key, session=session, target_datetime=target_datetime, logger=logger)\n",
    "    df = df.loc[~df['PRICE'].isna()]  # Only keep prices that are defined\n",
    "    return [{\n",
    "        'datetime': arrow.get(dt.to_pydatetime()).datetime,\n",
    "        'price': sum_vector(row, ['PRICE']),  # currency / MWh\n",
    "        'currency': 'AUD',\n",
    "        'source': SOURCE,\n",
    "        'zoneKey': zone_key,\n",
    "    } for dt, row in df.iterrows()]\n",
    "\n",
    "\n",
    "def fetch_exchange(zone_key1, zone_key2, session=None, target_datetime=None, logger=logging.getLogger(__name__)) -> list:\n",
    "    sorted_zone_keys = sorted([zone_key1, zone_key2])\n",
    "    key = '->'.join(sorted_zone_keys)\n",
    "    df = fetch_main_df('power', sorted_zone_keys=sorted_zone_keys, session=session, target_datetime=target_datetime, logger=logger)\n",
    "    direction = EXCHANGE_MAPPING_DICTIONARY[key]['direction']\n",
    "\n",
    "    # Take the first column (there's only one)\n",
    "    series = df.iloc[:, 0]\n",
    "\n",
    "    return [{\n",
    "        'datetime': arrow.get(dt.to_pydatetime()).datetime,\n",
    "        'netFlow': value * direction,\n",
    "        'source': SOURCE,\n",
    "        'sortedZoneKeys': key,\n",
    "    } for dt, value in series.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime=datetime.datetime(2021, 10, 12, 9, 15, tzinfo=dateutil.tz.tzoffset(None, 36000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fetch_production('AUS-NSW', target_datetime=datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all = fetch_production('AUS-NSW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime=datetime.datetime(2020, 10, 12, 9, 15, tzinfo=dateutil.tz.tzoffset(None, 36000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_nsw_202010 = fetch_production('AUS-NSW', target_datetime=datetime)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5c0dbb994636821d5e79f4ae22dc0c4b504966093d72c3df7fa2f554525c7d9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('python37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
